
---

# Text Classification with Logistic Regression and Naive Bayes – Course Assignment 2

This repository contains the solution to Assignment 2 of the Natural Language Processing course. The assignment focuses on text classification using Logistic Regression and Naive Bayes models, implemented both from scratch and with standard machine learning libraries.

---

## Assignment Overview

The notebook is structured into two main questions:

### Question 1: Logistic Regression and Naive Bayes (From Scratch)

In this part, classical text classification models are implemented without relying on ready-made model implementations.

Main steps include:

- Text preprocessing and normalization
- Feature extraction using the Bag of Words representation
- Manual implementation of evaluation metrics
- From-scratch implementation of:
  - Logistic Regression
  - Naive Bayes
- Performance analysis and comparison of the models

The dataset consists of email texts labeled as spam or ham, where labels are mapped as:

- spam → 1
- ham → 0

---

### Question 2: Logistic Regression and Naive Bayes (Using Libraries)

In this part, standard machine learning libraries are allowed.

Key components:

- Extraction of structural features
- Extraction of statistical and content-based features
- Design of custom features
- Combination of all extracted features
- Training and evaluation of models using existing libraries such as scikit-learn

---

## Repository Structure

Repository name:
nlp-text-classification-ca2

---

# Text Classification with Logistic Regression and Naive Bayes – Course Assignment 2

This repository contains the solution to Assignment 2 of the Natural Language Processing course. The assignment focuses on text classification using Logistic Regression and Naive Bayes models, implemented both from scratch and with standard machine learning libraries.

---

## Assignment Overview

The notebook is structured into two main questions:

### Question 1: Logistic Regression and Naive Bayes (From Scratch)

In this part, classical text classification models are implemented without relying on ready-made model implementations.

Main steps include:

- Text preprocessing and normalization
- Feature extraction using the Bag of Words representation
- Manual implementation of evaluation metrics
- From-scratch implementation of:
  - Logistic Regression
  - Naive Bayes
- Performance analysis and comparison of the models

The dataset consists of email texts labeled as spam or ham, where labels are mapped as:

- spam → 1
- ham → 0

---

### Question 2: Logistic Regression and Naive Bayes (Using Libraries)

In this part, standard machine learning libraries are allowed.

Key components:

- Extraction of structural features
- Extraction of statistical and content-based features
- Design of custom features
- Combination of all extracted features
- Training and evaluation of models using existing libraries such as scikit-learn

---


## Key Concepts Covered

- Text preprocessing for machine learning
- Bag of Words feature representation
- Logistic Regression for text classification
- Naive Bayes classifiers
- Evaluation metrics for classification tasks
- Feature engineering in NLP

---



## Results and Analysis

All experimental results, evaluations, and analyses are fully documented inside the notebook.
